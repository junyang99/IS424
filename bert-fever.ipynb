{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffa1334c-8ccb-4212-9a04-a88232809a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: torch in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (3.4.0)\n",
      "Requirement already satisfied: scikit-learn in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: filelock in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: accelerate in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from accelerate) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from accelerate) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\n",
      "Requirement already satisfied: requests in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./jupyterlab-venv-pytorch-240/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy torch transformers datasets scikit-learn\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dcb8730-428b-4f3b-b18d-b9faf8010b7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting dataset loading process...\n",
      "Loading full dataset from train.jsonl...\n",
      "✅ Loaded full dataset: 145449 rows\n",
      "✅ Filtered dataset: 109810 rows (Only VERIFIABLE + SUPPORTS/REFUTES)\n",
      "Loading full dataset from shared_task_dev.jsonl...\n",
      "✅ Loaded full dataset: 19998 rows\n",
      "✅ Filtered dataset: 13332 rows (Only VERIFIABLE + SUPPORTS/REFUTES)\n",
      "✅ Datasets fully loaded and filtered!\n",
      "\n",
      "Mapping labels to binary values...\n",
      "✅ Label mapping complete!\n",
      "\n",
      "Extracting texts and labels for tokenization...\n",
      "✅ Text extraction complete!\n",
      "\n",
      "Initializing BERT tokenizer...\n",
      "Tokenizing datasets...\n",
      "✅ Tokenization complete!\n",
      "\n",
      "Loading BERT model with weighted loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/common/home/projectgrps/IS424/IS424G4/jupyterlab-venv-pytorch-240/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ BERT Model with Weighted Loss Loaded!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34320' max='34320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34320/34320 30:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.397600</td>\n",
       "      <td>0.463878</td>\n",
       "      <td>0.799655</td>\n",
       "      <td>0.805164</td>\n",
       "      <td>0.783615</td>\n",
       "      <td>0.827933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.276000</td>\n",
       "      <td>0.542983</td>\n",
       "      <td>0.799730</td>\n",
       "      <td>0.809612</td>\n",
       "      <td>0.771541</td>\n",
       "      <td>0.851635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.289600</td>\n",
       "      <td>0.699716</td>\n",
       "      <td>0.802355</td>\n",
       "      <td>0.815437</td>\n",
       "      <td>0.764814</td>\n",
       "      <td>0.873237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.292200</td>\n",
       "      <td>0.787990</td>\n",
       "      <td>0.797930</td>\n",
       "      <td>0.808855</td>\n",
       "      <td>0.767367</td>\n",
       "      <td>0.855086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.177500</td>\n",
       "      <td>1.083317</td>\n",
       "      <td>0.793579</td>\n",
       "      <td>0.812278</td>\n",
       "      <td>0.744809</td>\n",
       "      <td>0.893189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ BERT model training complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Load & Preprocess Data\n",
    "# ------------------------------\n",
    "\n",
    "def load_full_and_filter_data(file_path):\n",
    "    print(f\"Loading full dataset from {file_path}...\")\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_data = json.loads(line.strip())\n",
    "                data.append(json_data)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping invalid JSON: {line[:100]}\")\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"✅ Loaded full dataset: {len(df)} rows\")\n",
    "\n",
    "    # Apply filtering AFTER loading\n",
    "    df = df[(df[\"verifiable\"] == \"VERIFIABLE\") & (df[\"label\"].isin([\"SUPPORTS\", \"REFUTES\"]))]\n",
    "    print(f\"✅ Filtered dataset: {len(df)} rows (Only VERIFIABLE + SUPPORTS/REFUTES)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Paths\n",
    "train_file_path = \"train.jsonl\"\n",
    "dev_file_path = \"shared_task_dev.jsonl\"\n",
    "\n",
    "# Load and filter datasets **without predefined sample size**\n",
    "print(\"\\nStarting dataset loading process...\")\n",
    "train_df = load_full_and_filter_data(train_file_path)\n",
    "dev_df = load_full_and_filter_data(dev_file_path)\n",
    "print(\"✅ Datasets fully loaded and filtered!\")\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Label Conversion & Class Weighting\n",
    "# ------------------------------\n",
    "\n",
    "print(\"\\nMapping labels to binary values...\")\n",
    "label_map = {\"SUPPORTS\": 1, \"REFUTES\": 0}\n",
    "train_df['label'] = train_df['label'].map(label_map)\n",
    "dev_df['label'] = dev_df['label'].map(label_map)\n",
    "print(\"✅ Label mapping complete!\")\n",
    "\n",
    "# ✅ FIX: Compute class weights correctly using np.array\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.array([0, 1]), y=train_df['label'].values)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Tokenization & Dataset Conversion\n",
    "# ------------------------------\n",
    "\n",
    "print(\"\\nExtracting texts and labels for tokenization...\")\n",
    "train_texts, train_labels = train_df['claim'].tolist(), train_df['label'].tolist()\n",
    "dev_texts, dev_labels = dev_df['claim'].tolist(), dev_df['label'].tolist()\n",
    "print(\"✅ Text extraction complete!\")\n",
    "\n",
    "print(\"\\nInitializing BERT tokenizer...\")\n",
    "model_name = \"bert-base-uncased\"  # ✅ Using BERT instead of RoBERTa\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "dev_encodings = tokenizer(dev_texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "print(\"✅ Tokenization complete!\")\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": train_encodings[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": train_encodings[\"attention_mask\"].tolist(),\n",
    "    \"labels\": train_labels\n",
    "})\n",
    "dev_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": dev_encodings[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": dev_encodings[\"attention_mask\"].tolist(),\n",
    "    \"labels\": dev_labels\n",
    "})\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Load BERT Model (With Weighted Loss)\n",
    "# ------------------------------\n",
    "\n",
    "print(\"\\nLoading BERT model with weighted loss...\")\n",
    "class WeightedBERT(nn.Module):\n",
    "    def __init__(self, model_name, class_weights):\n",
    "        super().__init__()\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "bert_model = WeightedBERT(model_name, class_weights)\n",
    "print(\"\\n✅ BERT Model with Weighted Loss Loaded!\")\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Training Arguments & Trainer\n",
    "# ------------------------------\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,  \n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=2e-5,  # Optimized learning rate\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    fp16=True if torch.cuda.is_available() else False,  # 🔥 Mixed precision for faster training\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"\\n✅ BERT model training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4adc7-4b76-490a-9ed7-f0b23597f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "🔍 Interpretation of the Results\n",
    "1️⃣ Accuracy & F1-Score Remain Stable (~80%)\n",
    "The accuracy hovers around 79.3% - 80.2%, which is solid performance for a fact-verification task.\n",
    "The F1-score is also strong (0.81), meaning the model balances precision and recall effectively.\n",
    "2️⃣ High Recall but Declining Precision\n",
    "Your recall increased significantly (from 82.8% → 89.3%), meaning the model detects more true positives.\n",
    "However, precision dropped from 78.4% → 74.5%, meaning there are more false positives.\n",
    "This suggests that the model favors recall over precision, which may lead to more misclassified \"SUPPORTS\" predictions.\n",
    "3️⃣ Training Loss is Low, but Validation Loss is Increasing\n",
    "Training loss decreases significantly from 0.39 → 0.18, showing the model is learning.\n",
    "However, validation loss starts increasing after Epoch 2 (0.46 → 1.08).\n",
    "This suggests overfitting, where the model memorizes the training data instead of generalizing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac720d-d1b3-443a-b4dc-cf6a36d6f734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421edc07-9fa3-4523-96f0-89b6ffe9ab3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "361f8f92-aef0-4246-9665-1114ccc0b185",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting dataset loading process...\n",
      "Loading full dataset from train.jsonl...\n",
      "✅ Loaded full dataset: 145449 rows\n",
      "✅ Filtered dataset: 109810 rows (Only VERIFIABLE + SUPPORTS/REFUTES)\n",
      "Loading full dataset from shared_task_dev.jsonl...\n",
      "✅ Loaded full dataset: 19998 rows\n",
      "✅ Filtered dataset: 13332 rows (Only VERIFIABLE + SUPPORTS/REFUTES)\n",
      "✅ Datasets fully loaded and filtered!\n",
      "\n",
      "Mapping labels to binary values...\n",
      "✅ Label mapping complete!\n",
      "✅ Text extraction complete!\n",
      "\n",
      "✅ Computed Class Weights: tensor([1.8440, 0.6860])\n",
      "\n",
      "Initializing BERT tokenizer...\n",
      "Tokenizing datasets...\n",
      "✅ Tokenization complete!\n",
      "\n",
      "Loading BERT model with weighted loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/common/home/projectgrps/IS424/IS424G4/jupyterlab-venv-pytorch-240/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERT Model with Weighted Loss Loaded!\n",
      "\n",
      "Configuring training arguments...\n",
      "✅ Training arguments configured!\n",
      "\n",
      "Starting BERT model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='41181' max='41181' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [41181/41181 33:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.498700</td>\n",
       "      <td>0.601804</td>\n",
       "      <td>0.792379</td>\n",
       "      <td>0.811367</td>\n",
       "      <td>0.743382</td>\n",
       "      <td>0.893039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.467100</td>\n",
       "      <td>0.762295</td>\n",
       "      <td>0.788104</td>\n",
       "      <td>0.813544</td>\n",
       "      <td>0.726341</td>\n",
       "      <td>0.924542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.877682</td>\n",
       "      <td>0.792379</td>\n",
       "      <td>0.814726</td>\n",
       "      <td>0.735557</td>\n",
       "      <td>0.912991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ BERT model training complete!\n",
      "\n",
      "Evaluating BERT model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1667' max='1667' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1667/1667 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Final Evaluation Results: {'eval_loss': 0.8776822686195374, 'eval_accuracy': 0.7923792379237924, 'eval_f1': 0.8147255689424364, 'eval_precision': 0.7355571670292482, 'eval_recall': 0.912991299129913, 'eval_runtime': 19.2432, 'eval_samples_per_second': 692.818, 'eval_steps_per_second': 86.628, 'epoch': 3.0}\n",
      "\n",
      "🚀 Script execution complete! BERT model is trained and evaluated successfully! ✅\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Load Full Dataset and Apply Filtering\n",
    "# ------------------------------\n",
    "def load_full_and_filter_data(file_path):\n",
    "    \"\"\" Load JSONL dataset, filter only verifiable claims (SUPPORTS & REFUTES). \"\"\"\n",
    "    print(f\"Loading full dataset from {file_path}...\")\n",
    "    data = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_data = json.loads(line.strip())\n",
    "                data.append(json_data)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping invalid JSON: {line[:100]}\")\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"✅ Loaded full dataset: {len(df)} rows\")\n",
    "\n",
    "    # Apply filtering AFTER loading\n",
    "    df = df[(df[\"verifiable\"] == \"VERIFIABLE\") & (df[\"label\"].isin([\"SUPPORTS\", \"REFUTES\"]))]\n",
    "    print(f\"✅ Filtered dataset: {len(df)} rows (Only VERIFIABLE + SUPPORTS/REFUTES)\")\n",
    "\n",
    "    return df  # Return full filtered dataset\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Load and Preprocess Data\n",
    "# ------------------------------\n",
    "train_file_path = \"train.jsonl\"\n",
    "dev_file_path = \"shared_task_dev.jsonl\"\n",
    "\n",
    "print(\"\\nStarting dataset loading process...\")\n",
    "train_df = load_full_and_filter_data(train_file_path)\n",
    "dev_df = load_full_and_filter_data(dev_file_path)\n",
    "print(\"✅ Datasets fully loaded and filtered!\")\n",
    "\n",
    "# Convert labels\n",
    "print(\"\\nMapping labels to binary values...\")\n",
    "label_map = {\"SUPPORTS\": 1, \"REFUTES\": 0}\n",
    "train_df['label'] = train_df['label'].map(label_map)\n",
    "dev_df['label'] = dev_df['label'].map(label_map)\n",
    "print(\"✅ Label mapping complete!\")\n",
    "\n",
    "# Extract text and labels\n",
    "train_texts, train_labels = train_df['claim'].tolist(), train_df['label'].tolist()\n",
    "dev_texts, dev_labels = dev_df['claim'].tolist(), dev_df['label'].tolist()\n",
    "print(\"✅ Text extraction complete!\")\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Compute Class Weights (Balances Precision & Recall)\n",
    "# ------------------------------\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", \n",
    "    classes=np.array([0, 1]),  # 🔥 Fix: Convert classes to NumPy array\n",
    "    y=train_df['label'].values\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)  # Convert back to tensor for PyTorch\n",
    "print(\"\\n✅ Computed Class Weights:\", class_weights)\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Tokenization\n",
    "# ------------------------------\n",
    "print(\"\\nInitializing BERT tokenizer...\")\n",
    "model_name = \"bert-base-uncased\"  # Still using BERT, but we can test RoBERTa later\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "dev_encodings = tokenizer(dev_texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "print(\"✅ Tokenization complete!\")\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": train_encodings[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": train_encodings[\"attention_mask\"].tolist(),\n",
    "    \"labels\": train_labels\n",
    "})\n",
    "dev_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": dev_encodings[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": dev_encodings[\"attention_mask\"].tolist(),\n",
    "    \"labels\": dev_labels\n",
    "})\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Load BERT Model with Weighted Loss\n",
    "# ------------------------------\n",
    "class WeightedBERT(nn.Module):\n",
    "    \"\"\" Custom BERT Model with Weighted Loss. \"\"\"\n",
    "    def __init__(self, model_name, num_labels, class_weights):\n",
    "        super().__init__()\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights)  # Apply weighted loss\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "print(\"\\nLoading BERT model with weighted loss...\")\n",
    "bert_model = WeightedBERT(model_name, num_labels=2, class_weights=class_weights)\n",
    "print(\"✅ BERT Model with Weighted Loss Loaded!\")\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Training Arguments (Optimized for Speed & Stability)\n",
    "# ------------------------------\n",
    "print(\"\\nConfiguring training arguments...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Match eval and save strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,  # Lowered batch size for stability\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,  # 🔥 Reduced epochs to prevent overfitting\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=1e-5,  # 🔥 Lowered learning rate for better precision\n",
    "    lr_scheduler_type=\"linear\",  # Uses learning rate scheduler\n",
    "    fp16=True if torch.cuda.is_available() else False,  # Enable mixed precision if on GPU\n",
    "    dataloader_num_workers=2,  # Multi-threaded data loading\n",
    "    logging_dir=\"./logs\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "print(\"✅ Training arguments configured!\")\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Compute Metrics\n",
    "# ------------------------------\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Train BERT Model with Early Stopping\n",
    "# ------------------------------\n",
    "print(\"\\nStarting BERT model training...\")\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"\\n✅ BERT model training complete!\")\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Evaluate Model\n",
    "# ------------------------------\n",
    "print(\"\\nEvaluating BERT model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"📊 Final Evaluation Results: {eval_results}\")\n",
    "\n",
    "print(\"\\n🚀 Script execution complete! BERT model is trained and evaluated successfully! ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb49071-abb5-4766-9f09-ba8d906277df",
   "metadata": {},
   "outputs": [],
   "source": [
    "🔥 Lower Validation Loss (From 1.0833 → 0.8777)\n",
    "\n",
    "This suggests that the model is less overfitting compared to before.\n",
    "A lower loss means the model is better at generalizing to new data.\n",
    "✅ Slightly Higher F1 Score (From 0.8123 → 0.8147)\n",
    "\n",
    "F1 score is a balance of precision and recall, so the slight increase is a good sign.\n",
    "⚡ Higher Recall (From 0.8932 → 0.9130)\n",
    "\n",
    "The new model is doing better at identifying \"SUPPORTS\" and \"REFUTES\" cases, meaning it's less likely to miss relevant instances.\n",
    "🔻 Small Drop in Precision (From 0.7448 → 0.7356)\n",
    "\n",
    "A small decrease in precision suggests that while the model is catching more correct instances (higher recall), it's also introducing a few more false positives.\n",
    "This is expected when using class weighting, as it favors the minority class.\n",
    "\n",
    "YES! ✅ While accuracy is almost the same, the lower validation loss, higher recall, and slightly improved F1 score indicate that the model is learning better and making more balanced predictions. The precision drop is minor and acceptable given the recall improvements.\n",
    "\n",
    "3️⃣ Implications of Results\n",
    "✅ Lower validation loss in the second model suggests that the model learns efficiently in fewer epochs, avoiding overfitting. ✅ Higher recall in the second model means it's better at catching \"REFUTES\" claims, which is important to ensure misinformation is flagged. ✅ Class weighting (1.84:0.68) effectively balanced label representation without introducing bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6965c311-82ea-4a07-b542-12748dfb4a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3398126-cc7c-4700-a2b6-a80d9c42dbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3988ebe2-a155-4895-8b03-b0ff366aad68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting dataset loading process...\n",
      "Loading full dataset from train.jsonl...\n",
      "✅ Loaded full dataset: 145449 rows\n",
      "✅ Filtered dataset: 109810 rows (Only VERIFIABLE + SUPPORTS/REFUTES)\n",
      "Loading full dataset from shared_task_dev.jsonl...\n",
      "✅ Loaded full dataset: 19998 rows\n",
      "✅ Filtered dataset: 13332 rows (Only VERIFIABLE + SUPPORTS/REFUTES)\n",
      "✅ Datasets fully loaded and filtered!\n",
      "\n",
      "Mapping labels to binary values...\n",
      "✅ Label mapping complete!\n",
      "\n",
      "✅ Computed Class Weights: tensor([1.8440, 0.6860], device='cuda:0')\n",
      "\n",
      "Initializing BERT tokenizer...\n",
      "Tokenizing datasets...\n",
      "✅ Tokenization complete!\n",
      "\n",
      "Loading BERT model with weighted loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERT Model Loaded on cuda\n",
      "\n",
      "Configuring training arguments...\n",
      "✅ Training arguments configured!\n",
      "\n",
      "Starting BERT model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/projectgrps/IS424/IS424G4/jupyterlab-venv-pytorch-240/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10296' max='13728' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10296/13728 16:23 < 05:27, 10.47 it/s, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.388200</td>\n",
       "      <td>0.432852</td>\n",
       "      <td>0.801155</td>\n",
       "      <td>0.802621</td>\n",
       "      <td>0.796748</td>\n",
       "      <td>0.808581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.280500</td>\n",
       "      <td>0.518925</td>\n",
       "      <td>0.802880</td>\n",
       "      <td>0.810362</td>\n",
       "      <td>0.780729</td>\n",
       "      <td>0.842334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.244900</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.803555</td>\n",
       "      <td>0.815238</td>\n",
       "      <td>0.769477</td>\n",
       "      <td>0.866787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ BERT model training complete!\n",
      "\n",
      "Evaluating BERT model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='834' max='834' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [834/834 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Final Evaluation Results: {'eval_loss': 0.43285226821899414, 'eval_accuracy': 0.8011551155115512, 'eval_f1': 0.8026208026208026, 'eval_precision': 0.7967479674796748, 'eval_recall': 0.8085808580858086, 'eval_runtime': 9.6332, 'eval_samples_per_second': 1383.96, 'eval_steps_per_second': 86.575, 'epoch': 3.0}\n",
      "\n",
      "🚀 Script execution complete! Final Optimized BERT model trained successfully! ✅\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch.nn as nn\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Load and Filter Dataset\n",
    "# ------------------------------\n",
    "def load_full_and_filter_data(file_path):\n",
    "    print(f\"Loading full dataset from {file_path}...\")\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_data = json.loads(line.strip())\n",
    "                data.append(json_data)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping invalid JSON: {line[:100]}\")\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"✅ Loaded full dataset: {len(df)} rows\")\n",
    "\n",
    "    # Filtering to include only VERIFIABLE claims with SUPPORTS or REFUTES labels\n",
    "    df = df[(df[\"verifiable\"] == \"VERIFIABLE\") & (df[\"label\"].isin([\"SUPPORTS\", \"REFUTES\"]))]\n",
    "    print(f\"✅ Filtered dataset: {len(df)} rows (Only VERIFIABLE + SUPPORTS/REFUTES)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Paths\n",
    "train_file_path = \"train.jsonl\"\n",
    "dev_file_path = \"shared_task_dev.jsonl\"\n",
    "\n",
    "# Load and filter datasets\n",
    "print(\"\\nStarting dataset loading process...\")\n",
    "train_df = load_full_and_filter_data(train_file_path)\n",
    "dev_df = load_full_and_filter_data(dev_file_path)\n",
    "print(\"✅ Datasets fully loaded and filtered!\")\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Label Encoding\n",
    "# ------------------------------\n",
    "print(\"\\nMapping labels to binary values...\")\n",
    "label_map = {\"SUPPORTS\": 1, \"REFUTES\": 0}\n",
    "train_df['label'] = train_df['label'].map(label_map)\n",
    "dev_df['label'] = dev_df['label'].map(label_map)\n",
    "print(\"✅ Label mapping complete!\")\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Compute Class Weights (Balances Precision & Recall)\n",
    "# ------------------------------\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", \n",
    "    classes=np.array([0, 1]),  # Ensure correct format\n",
    "    y=train_df['label'].values\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "print(\"\\n✅ Computed Class Weights:\", class_weights)\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Tokenization\n",
    "# ------------------------------\n",
    "print(\"\\nInitializing BERT tokenizer...\")\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_encodings = tokenizer(train_df[\"claim\"].tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "dev_encodings = tokenizer(dev_df[\"claim\"].tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "print(\"✅ Tokenization complete!\")\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": train_encodings[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": train_encodings[\"attention_mask\"].tolist(),\n",
    "    \"labels\": train_df['label'].tolist()\n",
    "})\n",
    "dev_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": dev_encodings[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": dev_encodings[\"attention_mask\"].tolist(),\n",
    "    \"labels\": dev_df['label'].tolist()\n",
    "})\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Load BERT Model\n",
    "# ------------------------------\n",
    "print(\"\\nLoading BERT model with weighted loss...\")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "bert_model.to(device)\n",
    "print(\"✅ BERT Model Loaded on\", device)\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Custom Loss Function (Weighted Cross Entropy)\n",
    "# ------------------------------\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):  # FIX: added num_items_in_batch\n",
    "        labels = inputs.pop(\"labels\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute loss with class weights\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Training Arguments (Final Optimized Settings)\n",
    "# ------------------------------\n",
    "print(\"\\nConfiguring training arguments...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\", \n",
    "    per_device_train_batch_size=16,  # 🔥 Sticking to batch size 16 for stability\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,  # Increased to 4\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=3e-5,  # Increased learning rate\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"linear\",  # Uses learning rate scheduler\n",
    "    gradient_accumulation_steps=2,  # Simulate larger batch size\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "print(\"✅ Training arguments configured!\")\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Train BERT Model with Early Stopping\n",
    "# ------------------------------\n",
    "print(\"\\nStarting BERT model training...\")\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=lambda pred: {\n",
    "        \"accuracy\": accuracy_score(pred.label_ids, pred.predictions.argmax(-1)),\n",
    "        \"f1\": precision_recall_fscore_support(pred.label_ids, pred.predictions.argmax(-1), average='binary')[2],\n",
    "        \"precision\": precision_recall_fscore_support(pred.label_ids, pred.predictions.argmax(-1), average='binary')[0],\n",
    "        \"recall\": precision_recall_fscore_support(pred.label_ids, pred.predictions.argmax(-1), average='binary')[1],\n",
    "    },\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Stops if no improvement for 2 epochs\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"\\n✅ BERT model training complete!\")\n",
    "\n",
    "# ------------------------------\n",
    "# ✅ Evaluate Model\n",
    "# ------------------------------\n",
    "print(\"\\nEvaluating BERT model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"📊 Final Evaluation Results: {eval_results}\")\n",
    "\n",
    "print(\"\\n🚀 Script execution complete! Final Optimized BERT model trained successfully! ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dfdb1c-7c2d-444d-97e6-7f31220b28de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Observations:\n",
    "\n",
    "Eval Loss: The model's final validation loss is relatively low (0.4328), indicating that the model has learned effectively without overfitting.\n",
    "Accuracy (80.1%): This is significantly higher than the previous XGBoost models, which hovered around 64-68% accuracy.\n",
    "Precision & Recall: The precision (79.67%) and recall (80.86%) indicate a strong balance, meaning the model is not biased toward one class.\n",
    "F1-score (80.26%): A well-balanced score shows the model is performing robustly for both \"SUPPORTS\" and \"REFUTES\" categories.\n",
    "\n",
    "\n",
    "🔍 What Did We Improve Compared to Previous Runs?\n",
    "✅ Balanced Data Handling: We used class weights (tensor([1.8440, 0.6860])) to account for label imbalance, improving recall and F1-score.\n",
    "✅ GPU Utilization: Training was significantly faster (using CUDA) compared to CPU runs.\n",
    "✅ Optimized Hyperparameters:\n",
    "\n",
    "Batch Size: Maintained 16, ensuring stability while fully utilizing GPU.\n",
    "Gradient Accumulation Steps: Effectively simulated a larger batch size.\n",
    "Learning Rate: 3e-5, providing a stable and effective convergence rate.\n",
    "Early Stopping: Prevented unnecessary training epochs, ensuring the best model was saved.\n",
    "✅ Lower Validation Loss: The validation loss remained low (0.4328) and did not significantly diverge from training loss, indicating a well-generalized model.\n",
    "\n",
    "\n",
    "\n",
    "Why Did the Model Stop Improving?\n",
    "Validation Loss Increased After Epoch 1\n",
    "\n",
    "Epoch 1 had the lowest validation loss (0.4328).\n",
    "Epoch 2 & 3 saw an increase in validation loss, meaning the model might be overfitting.\n",
    "Accuracy & F1-score Remained Nearly Constant\n",
    "\n",
    "After Epoch 1, accuracy only increased from 80.1% → 80.3%, a very minor gain.\n",
    "F1-score improved slightly (from 80.26% → 81.52%), but the improvement was marginal.\n",
    "Overfitting Detected?\n",
    "\n",
    "The training loss kept decreasing, but validation loss increased, which is a sign of overfitting.\n",
    "Precision declined slightly while recall increased, meaning the model started favoring recall over balanced performance.\n",
    "✔ Epoch 1 was the most optimal, and additional training did not improve generalization.\n",
    "✔ The early stopping mechanism worked correctly, preventing unnecessary epochs.\n",
    "✔ Best Model Saved at Epoch 1, even though training continued.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4571d56b-24df-46cc-8522-2f1cee42a219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358687e-7d46-455b-8f38-3cbaf99b6693",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final Conclusion\n",
    "Your BERT model is a clear winner over XGBoost, achieving 80.1% accuracy and a well-balanced F1-score of 80.26%. By leveraging pretrained embeddings, GPU acceleration, and class weighting, we significantly enhanced generalization and performance.\n",
    "\n",
    "This experiment confirms that deep learning models like BERT are far superior for text classification tasks involving nuanced claims, and with further fine-tuning, it can potentially exceed 85% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa6a1d-c0d7-4e12-bbf0-149da0803e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT vs. XGBoost – Final Comparison\n",
    "Model\tAccuracy\tPrecision\tRecall\tF1-score\n",
    "BERT (Final Run)\t80.1%\t79.67%\t80.86%\t80.26%\n",
    "XGBoost (Best Run)\t67.7%\t70.83%\t67.23%\t65.84%\n",
    "🔹 Key Takeaways from the Comparison:\n",
    "\n",
    "BERT significantly outperforms XGBoost across all metrics, showing the strength of deep learning over traditional ML.\n",
    "XGBoost models relied heavily on TF-IDF, Word2Vec, and FastText embeddings, but they couldn't capture contextual meaning as well as BERT.\n",
    "Recall in BERT (80.86%) is much higher than XGBoost (~67%), meaning fewer false negatives, making it better at correctly classifying both SUPPORTS and REFUTES.\n",
    "XGBoost models, even with ADASYN and SMOTE, struggled with handling class balance as effectively as BERT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
